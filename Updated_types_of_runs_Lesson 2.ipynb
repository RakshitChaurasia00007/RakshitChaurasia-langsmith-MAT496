{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5NckRFun5Gv"
      },
      "source": [
        "# Tracing for Different Types of Runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiQ6ApYtn5Gz"
      },
      "source": [
        "### Types of Runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFS4y5Efn5G0"
      },
      "source": [
        "LangSmith supports many different types of Runs - you can specify what type your Run is in the @traceable decorator. The types of runs are:\n",
        "\n",
        "- LLM: Invokes an LLM\n",
        "- Retriever: Retrieves documents from databases or other sources\n",
        "- Tool: Executes actions with function calls\n",
        "- Chain: Default type; combines multiple Runs into a larger process\n",
        "- Prompt: Hydrates a prompt to be used with an LLM\n",
        "- Parser: Extracts structured data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paZqR3xen5G1"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "yektRl5Xn5G2"
      },
      "outputs": [],
      "source": [
        "# You can set them inline!\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"OPEN AI API KEY TO BE USED \"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = \"LANGSMITH API KEY TO BE USED\"\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFegt05En5G4",
        "outputId": "b5726d49-f95c-463a-89d8-1fed5d4696d4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Or you can use a .env file\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(dotenv_path=\"../../.env\", override=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Cqn-qVzn5G4"
      },
      "source": [
        "### LLM Runs for Chat Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrH3zQhCn5G5"
      },
      "source": [
        "LangSmith provides special rendering and processing for LLM traces. In order to make the most of this feature, you must log your LLM traces in a specific format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3IT07j0n5G6"
      },
      "source": [
        "For chat-style models, inputs must be a list of messages in OpenAI-compatible format, represented as Python dictionaries or TypeScript object. Each message must contain the key role and content.\n",
        "\n",
        "The output is accepted in any of the following formats:\n",
        "\n",
        "- A dictionary/object that contains the key choices with a value that is a list of dictionaries/objects. Each dictionary/object must contain the key message, which maps to a message object with the keys role and content.\n",
        "- A dictionary/object that contains the key message with a value that is a message object with the keys role and content.\n",
        "- A tuple/array of two elements, where the first element is the role and the second element is the content.\n",
        "- A dictionary/object that contains the key role and content.\n",
        "The input to your function should be named messages.\n",
        "\n",
        "You can also provide the following metadata fields to help LangSmith identify the model and calculate costs. If using LangChain or OpenAI wrapper, these fields will be automatically populated correctly.\n",
        "- ls_provider: The provider of the model, eg \"openai\", \"anthropic\", etc.\n",
        "- ls_model_name: The name of the model, eg \"gpt-4o-mini\", \"claude-3-opus-20240307\", etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdK8kQGqn5G6",
        "outputId": "6ebf70aa-0d3d-4674-ed06-267d1120dc8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chat model received messages: [{'role': 'system', 'content': 'You are a knowledgeable assistant.'}, {'role': 'user', 'content': 'Can you help me book a table for two people?'}]\n",
            "Chat model response: {'choices': [{'message': {'role': 'assistant', 'content': 'Absolutely! For what time should I book the table?'}}]}\n"
          ]
        }
      ],
      "source": [
        "from langsmith import traceable\n",
        "\n",
        "# Use API key (set in environment as OPENAI_API_KEY and LANGSMITH_API_KEY)\n",
        "\n",
        "inputs = [\n",
        "  {\"role\": \"system\", \"content\": \"You are a knowledgeable assistant.\"},\n",
        "  {\"role\": \"user\", \"content\": \"Can you help me book a table for two people?\"},\n",
        "]\n",
        "\n",
        "output = {\n",
        "  \"choices\": [\n",
        "      {\n",
        "          \"message\": {\n",
        "              \"role\": \"assistant\",\n",
        "              \"content\": \"Absolutely! For what time should I book the table?\"\n",
        "          }\n",
        "      }\n",
        "  ]\n",
        "}\n",
        "\n",
        "@traceable(\n",
        "  run_type=\"llm\",\n",
        "  metadata={\"ls_provider\": \"custom_provider\", \"ls_model_name\": \"custom-model-v1\"}\n",
        ")\n",
        "def chat_model(messages: list):\n",
        "  print(f\"Chat model received messages: {messages}\")  # Added debug log\n",
        "  return output\n",
        "\n",
        "result = chat_model(inputs)\n",
        "print(\"Chat model response:\", result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YBn1zsen5G8"
      },
      "source": [
        "### Handling Streaming LLM Runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpXWXpY9n5G8"
      },
      "source": [
        "For streaming, you can \"reduce\" the outputs into the same format as the non-streaming version. This is currently only supported in Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPtkKX_Rn5G9",
        "outputId": "8a7cf6d3-0a4d-4c13-8f8c-3acb1ccc831c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streaming chunk: Hello, polly the parrot\n",
            "Streaming chunk:  How can I assist you today?\n",
            "Concatenated streaming response: [{'choices': [{'message': {'content': 'Hello, polly the parrot', 'role': 'assistant'}}]}, {'choices': [{'message': {'content': ' How can I assist you today?', 'role': 'assistant'}}]}]\n"
          ]
        }
      ],
      "source": [
        "from langsmith import traceable\n",
        "\n",
        "# Use API key (set in environment as OPENAI_API_KEY and LANGSMITH_API_KEY)\n",
        "\n",
        "def _reduce_chunks(chunks: list):\n",
        "    concatenated = \"\".join([chunk[\"choices\"][0][\"message\"][\"content\"] for chunk in chunks])\n",
        "    return {\"choices\": [{\"message\": {\"content\": concatenated, \"role\": \"assistant\"}}]}\n",
        "\n",
        "@traceable(\n",
        "    run_type=\"llm\",\n",
        "    metadata={\"ls_provider\": \"custom_provider\", \"ls_model_name\": \"custom-model-v1\"},\n",
        "    reduce_fn=_reduce_chunks\n",
        ")\n",
        "def streaming_chat(messages: list):\n",
        "    greeting = \"Hello, \" + messages[1].get(\"content\", \"\")\n",
        "    for partial in [greeting, \" How can I assist you today?\"]:\n",
        "        print(f\"Streaming chunk: {partial}\")  # Debug print\n",
        "        yield {\n",
        "            \"choices\": [\n",
        "                {\n",
        "                    \"message\": {\n",
        "                        \"content\": partial,\n",
        "                        \"role\": \"assistant\",\n",
        "                    }\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "collected_chunks = list(\n",
        "    streaming_chat(\n",
        "        [\n",
        "            {\"role\": \"system\", \"content\": \"You act as an attentive assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": \"polly the parrot\"},\n",
        "        ]\n",
        "    )\n",
        ")\n",
        "print(\"Concatenated streaming response:\", collected_chunks)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-F20HGjn5G-"
      },
      "source": [
        "### Retriever Runs + Documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gI2XgtAfn5G-"
      },
      "source": [
        "Many LLM applications require looking up documents from vector databases, knowledge graphs, or other types of indexes. Retriever traces are a way to log the documents that are retrieved by the retriever. LangSmith provides special rendering for retrieval steps in traces to make it easier to understand and diagnose retrieval issues. In order for retrieval steps to be rendered correctly, a few small steps need to be taken.\n",
        "\n",
        "1. Annotate the retriever step with run_type=\"retriever\".\n",
        "2. Return a list of Python dictionaries or TypeScript objects from the retriever step. Each dictionary should contain the following keys:\n",
        "    - page_content: The text of the document.\n",
        "    - type: This should always be \"Document\".\n",
        "    - metadata: A python dictionary or TypeScript object containing metadata about the document. This metadata will be displayed in the trace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVxYQjJhn5G_",
        "outputId": "40ea90c7-18eb-45e8-94e1-659f2c8a1a78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retriever called with query: Tracing basics explanation\n",
            "Retrieved documents: [{'page_content': 'Doc 1: Insights on LangSmith tracing.', 'type': 'Document', 'metadata': {'source': \"Source for 'Doc 1: Ins...'\"}}, {'page_content': 'Doc 2: Introduction to Python decorators.', 'type': 'Document', 'metadata': {'source': \"Source for 'Doc 2: Int...'\"}}, {'page_content': 'Doc 3: Handling API keys securely.', 'type': 'Document', 'metadata': {'source': \"Source for 'Doc 3: Han...'\"}}]\n"
          ]
        }
      ],
      "source": [
        "from langsmith import traceable\n",
        "\n",
        "# Use API key (set in environment as OPENAI_API_KEY and LANGSMITH_API_KEY)\n",
        "\n",
        "def format_documents(results):\n",
        "    return [\n",
        "        {\n",
        "            \"page_content\": result,\n",
        "            \"type\": \"Document\",\n",
        "            \"metadata\": {\"source\": f\"Source for '{result[:10]}...'\"}\n",
        "        }\n",
        "        for result in results\n",
        "    ]\n",
        "\n",
        "@traceable(run_type=\"retriever\")\n",
        "def search_documents(query: str):\n",
        "    sample_docs = [\n",
        "        \"Doc 1: Insights on LangSmith tracing.\",\n",
        "        \"Doc 2: Introduction to Python decorators.\",\n",
        "        \"Doc 3: Handling API keys securely.\"\n",
        "    ]\n",
        "    print(f\"Retriever called with query: {query}\")  # Debug log\n",
        "    return format_documents(sample_docs)\n",
        "\n",
        "retrieved_docs = search_documents(\"Tracing basics explanation\")\n",
        "print(\"Retrieved documents:\", retrieved_docs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkU3DKpvn5G_"
      },
      "source": [
        "### Tool Calling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1CmMbVZn5G_"
      },
      "source": [
        "LangSmith has custom rendering for Tool Calls made by the model to make it clear when provided tools are being used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72RfGiEtn5G_",
        "outputId": "1aaed6b8-3f9e-4514-e937-eea8b6603e09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=14b5d65a-875a-49c6-b1a7-ca793a16950a,id=14b5d65a-875a-49c6-b1a7-ca793a16950a; trace=14b5d65a-875a-49c6-b1a7-ca793a16950a,id=fc85be47-3286-4622-9df0-3dc384ed6950\n",
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=14b5d65a-875a-49c6-b1a7-ca793a16950a,id=fc85be47-3286-4622-9df0-3dc384ed6950; trace=14b5d65a-875a-49c6-b1a7-ca793a16950a,id=a3baf172-2465-42a3-ac4a-dd3f17c0ffa7; trace=14b5d65a-875a-49c6-b1a7-ca793a16950a,id=a3baf172-2465-42a3-ac4a-dd3f17c0ffa7; trace=14b5d65a-875a-49c6-b1a7-ca793a16950a,id=41323ede-39f1-4aea-a3e3-1d7d75e89d9f\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-CMxrlMebXgMvbH4nF4MINcpPLyBTe', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The current temperature in New York City is 65 degrees Fahrenheit. If you need more detailed weather information, feel free to ask!', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1759589377, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_51db84afab', usage=CompletionUsage(completion_tokens=26, prompt_tokens=96, total_tokens=122, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ],
      "source": [
        "# Before running this script, set your API keys as environment variables in your terminal or shell:\n",
        "# export OPENAI_API_KEY=\"your_openai_api_key_here\"\n",
        "# export LANGSMITH_API_KEY=\"your_langsmith_api_key_here\"\n",
        "\n",
        "from langsmith import traceable\n",
        "from openai import OpenAI\n",
        "from typing import List, Optional\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Alternatively, you can set environment variables here directly (not recommended for production)\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key_here\"\n",
        "# os.environ[\"LANGSMITH_API_KEY\"] = \"your_langsmith_api_key_here\"\n",
        "\n",
        "openai_client = OpenAI()\n",
        "\n",
        "@traceable(run_type=\"tool\")\n",
        "def get_current_temperature(location: str, unit: str):\n",
        "    temperature = 65 if unit == \"Fahrenheit\" else 17\n",
        "    # Changed output string format\n",
        "    return f\"The current temperature in {location} is {temperature} degrees {unit}.\"\n",
        "\n",
        "@traceable(run_type=\"llm\")\n",
        "def call_openai(messages: List[dict], tools: Optional[List[dict]]) -> str:\n",
        "    return openai_client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=messages,\n",
        "        temperature=0,\n",
        "        tools=tools\n",
        "    )\n",
        "\n",
        "@traceable(run_type=\"chain\")\n",
        "def ask_about_the_weather(inputs, tools):\n",
        "    response = call_openai(inputs, tools)\n",
        "    tool_call_args = json.loads(response.choices[0].message.tool_calls[0].function.arguments)\n",
        "    location = tool_call_args[\"location\"]\n",
        "    unit = tool_call_args[\"unit\"]\n",
        "\n",
        "    tool_response_message = {\n",
        "        \"role\": \"tool\",\n",
        "        \"content\": json.dumps({\n",
        "            \"message\": get_current_temperature(location, unit),\n",
        "            \"location\": location,\n",
        "            \"unit\": unit\n",
        "        }),\n",
        "        \"tool_call_id\": response.choices[0].message.tool_calls[0].id\n",
        "    }\n",
        "\n",
        "    inputs.append(response.choices[0].message)\n",
        "    inputs.append(tool_response_message)\n",
        "    output = call_openai(inputs, None)\n",
        "    return output\n",
        "\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_current_temperature\",\n",
        "            \"description\": \"Get the current temperature for a specific location\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"location\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The city and state, e.g., San Francisco, CA\"\n",
        "                    },\n",
        "                    \"unit\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"enum\": [\"Celsius\", \"Fahrenheit\"],\n",
        "                        \"description\": \"The temperature unit to use. Infer this from the user's location.\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"location\", \"unit\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "inputs = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What is the weather today in New York City?\"},\n",
        "]\n",
        "\n",
        "result = ask_about_the_weather(inputs, tools)\n",
        "print(result)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ls-academy",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}