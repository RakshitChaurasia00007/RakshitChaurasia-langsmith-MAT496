{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_alY4x8FlF6"
      },
      "source": [
        "# Alternative Tracing Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHlzecIfFlF9"
      },
      "source": [
        "![AWTT](../../images/alternative_ways_to_trace_0.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxMbi4SWFlF-"
      },
      "source": [
        "So far in this module, we've taken a look at the traceable decorator, and how we can use it to set up tracing.\n",
        "\n",
        "In this lesson, we're going to look at alternative ways in which we can set up tracing, and when you should think about using these different approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG21PCCOFlF-"
      },
      "source": [
        "## LangChain and LangGraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1lBQhHdFlF_"
      },
      "source": [
        "If we are using LangChain or LangGraph, all we need to do to set up tracing is to set a few environment variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7lGyJT2FlGA"
      },
      "source": [
        "![AWTT](../../images/alternative_ways_to_trace_1.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPawopF2FlGB"
      },
      "outputs": [],
      "source": [
        "# You can set them inline\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = \"\"\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy\"  # If you don't set this, traces will go to the Default project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1AOMyroFlGC",
        "outputId": "26f1faab-d922-4070-82ce-5ef27fb9fbd1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Or you can use a .env file\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(dotenv_path=\"../../.env\", override=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QWG0ymGFlGC"
      },
      "source": [
        "Don't worry too much about our graph implementation here, you can learn more about LangGraph through our LangGraph Academy course!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain openai langchain_community nest_asyncio\n"
      ],
      "metadata": {
        "id": "tJQBYOzOGnRO",
        "outputId": "fc305f2c-a151-4fb7-9253-53b11508bf1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Collecting openai\n",
            "  Downloading openai-2.1.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.30-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (1.6.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.77)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.31)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.9)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Collecting requests<3,>=2 (from langchain)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.11.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading openai-2.1.0-py3-none-any.whl (964 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m964.9/964.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.30-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: requests, mypy-extensions, marshmallow, typing-inspect, openai, dataclasses-json, langchain_community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.109.1\n",
            "    Uninstalling openai-1.109.1:\n",
            "      Successfully uninstalled openai-1.109.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 langchain_community-0.3.30 marshmallow-3.26.1 mypy-extensions-1.1.0 openai-2.1.0 requests-2.32.5 typing-inspect-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "requests"
                ]
              },
              "id": "ffeb46adeedf4f97b6a843b58ec74a84"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "UEy54U8yFlGD",
        "outputId": "4f07eae1-cd1b-469a-8ccb-c43acfc91067"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-1876171567.py, line 3)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1876171567.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    Sent from Mailspring\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-your-api-key-here\"  # <-- Add your API key here\n",
        "\n",
        "import nest_asyncio\n",
        "import operator\n",
        "from langchain.schema import Document, HumanMessage, BaseMessage\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from IPython.display import display\n",
        "from typing import List\n",
        "from typing_extensions import TypedDict, Annotated\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Dummy retriever\n",
        "class DummyDoc:\n",
        "    def __init__(self, page_content: str):\n",
        "        self.page_content = page_content\n",
        "\n",
        "class DummyRetriever:\n",
        "    def __init__(self):\n",
        "        self.documents = [\n",
        "            \"LangSmith helps trace and debug LLM applications.\",\n",
        "            \"OpenAI models like GPT-4o-mini can generate text and perform reasoning.\",\n",
        "            \"Vector databases are used in Retrieval-Augmented Generation (RAG) systems.\",\n",
        "            \"The @traceable decorator enables LangSmith function tracing.\",\n",
        "        ]\n",
        "\n",
        "    def invoke(self, query: str) -> List[DummyDoc]:\n",
        "        matched = [\n",
        "            DummyDoc(d) for d in self.documents\n",
        "            if any(word.lower() in d.lower() for word in query.split())\n",
        "        ]\n",
        "        return matched or [DummyDoc(d) for d in self.documents]\n",
        "\n",
        "def get_vector_db_retriever() -> DummyRetriever:\n",
        "    return DummyRetriever()\n",
        "\n",
        "RAG_PROMPT = \"\"\"Context:\\n{context}\\n\\nConversation:\\n{conversation}\\n\\nQuestion: {question}\"\"\"\n",
        "\n",
        "# Initialization\n",
        "nest_asyncio.apply()\n",
        "retriever = get_vector_db_retriever()\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "# Helper\n",
        "def get_buffer_string(messages: List[BaseMessage]) -> str:\n",
        "    return \"\\n\".join([f\"{m.__class__.__name__}: {m.content}\" for m in messages])\n",
        "\n",
        "# Graph state\n",
        "class GraphState(TypedDict):\n",
        "    question: str\n",
        "    messages: Annotated[List[BaseMessage], operator.add]\n",
        "    documents: List[Document]\n",
        "\n",
        "# Nodes\n",
        "def retrieve_documents(state: GraphState):\n",
        "    messages = state.get(\"messages\", [])\n",
        "    question = state[\"question\"]\n",
        "    documents = retriever.invoke(f\"{get_buffer_string(messages)} {question}\")\n",
        "    return {\"documents\": documents}\n",
        "\n",
        "def generate_response(state: GraphState):\n",
        "    question = state[\"question\"]\n",
        "    messages = state.get(\"messages\", [])\n",
        "    documents = state[\"documents\"]\n",
        "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
        "\n",
        "    rag_prompt_formatted = RAG_PROMPT.format(context=formatted_docs, conversation=messages, question=question)\n",
        "    generation = llm([HumanMessage(content=rag_prompt_formatted)])\n",
        "\n",
        "    return {\"documents\": documents, \"messages\": [HumanMessage(question), generation]}\n",
        "\n",
        "# Execute workflow\n",
        "state = {\"question\": \"Explain the @traceable decorator\", \"messages\": [], \"documents\": []}\n",
        "state.update(retrieve_documents(state))\n",
        "state.update(generate_response(state))\n",
        "\n",
        "# Print response\n",
        "print(\"AI Response:\")\n",
        "print(state[\"messages\"][-1].content)\n",
        "\n",
        "# Visualize workflow\n",
        "G = nx.DiGraph()\n",
        "G.add_edge(\"START\", \"retrieve_documents\")\n",
        "G.add_edge(\"retrieve_documents\", \"generate_response\")\n",
        "G.add_edge(\"generate_response\", \"END\")\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "nx.draw(G, with_labels=True, node_size=3000, node_color=\"lightblue\", arrows=True)\n",
        "plt.title(\"RAG Workflow Graph\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llxWpmvGFlGE"
      },
      "source": [
        "We're setting up a simple graph in LangGraph. If you want to learn more about LangGraph, I would highly recommend taking a look at our LangGraph Academy course.\n",
        "\n",
        "You can also pass in metadata or other fields through an optional config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "x32fd2hoFlGE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "outputId": "73a958bf-0890-49c0-c06b-d1d388156c4e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'simple_rag_graph' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1844875371.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"How do I set up tracing if I'm using LangChain?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msimple_rag_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"question\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"metadata\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"foo\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"bar\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'simple_rag_graph' is not defined"
          ]
        }
      ],
      "source": [
        "question = \"How do I set up tracing if I'm using LangChain?\"\n",
        "simple_rag_graph.invoke({\"question\": question}, config={\"metadata\": {\"foo\": \"bar\"}})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtDtR78mFlGF"
      },
      "source": [
        "##### Let's take a look in LangSmith!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLu5lwewFlGF"
      },
      "source": [
        "## Tracing Context Manager"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxQtXtzzFlGF"
      },
      "source": [
        "In Python, you can use the trace context manager to log traces to LangSmith. This is useful in situations where:\n",
        "\n",
        "You want to log traces for a specific block of code.\n",
        "You want control over the inputs, outputs, and other attributes of the trace.\n",
        "It is not feasible to use a decorator or wrapper.\n",
        "Any or all of the above.\n",
        "The context manager integrates seamlessly with the traceable decorator and wrap_openai wrapper, so you can use them together in the same application.\n",
        "\n",
        "You still need to set your `LANGSMITH_API_KEY` and `LANGSMITH_TRACING`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmOznhdqFlGF"
      },
      "source": [
        "![AWTT](../../images/alternative_ways_to_trace_2.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "iwi6VrSeFlGF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langsmith import trace, traceable\n",
        "from openai import OpenAI\n",
        "from typing import List\n",
        "import nest_asyncio\n",
        "from utils import get_vector_db_retriever\n",
        "\n",
        "# Load environment variables from your .env file\n",
        "load_dotenv(dotenv_path=\"../../.env\", override=True)\n",
        "\n",
        "# Set tracing environment variable\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"false\"  # using manual RunTree control\n",
        "# Ensure API keys are set in your environment or in your .env file\n",
        "\n",
        "# Initialize OpenAI client\n",
        "openai_client = OpenAI()\n",
        "nest_asyncio.apply()\n",
        "retriever = get_vector_db_retriever()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Fv7tkQ8SFlGG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "b6d4ba09-c55c-4605-c302-018f80e5d5ae"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'MODEL_NAME' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3878839442.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mtraceable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mcall_openai\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     response = openai_client.chat.completions.create(\n\u001b[1;32m     25\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'MODEL_NAME' is not defined"
          ]
        }
      ],
      "source": [
        "@traceable\n",
        "def retrieve_documents(question: str):\n",
        "    documents = retriever.invoke(question)\n",
        "    return documents\n",
        "\n",
        "@traceable\n",
        "def generate_response(question: str, documents):\n",
        "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": RAG_SYSTEM_PROMPT\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
        "        }\n",
        "    ]\n",
        "    response = call_openai(messages)\n",
        "    return response\n",
        "\n",
        "@traceable\n",
        "def call_openai(messages: List[dict], model: str = MODEL_NAME, temperature: float = 0.0):\n",
        "    response = openai_client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@traceable\n",
        "def langsmith_rag(question: str):\n",
        "    documents = retrieve_documents(question)\n",
        "    response = generate_response(question, documents)\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "question = \"How do I trace with tracing context?\"\n",
        "ai_answer = langsmith_rag(question)\n",
        "print(ai_answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "jqf_0bg4NeeN",
        "outputId": "9888cc55-a6c0-4488-f4b3-cc02dc2d52cf"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'RAG_SYSTEM_PROMPT' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2592218936.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"How do I trace with tracing context?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mai_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlangsmith_rag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mai_answer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2592218936.py\u001b[0m in \u001b[0;36mlangsmith_rag\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlangsmith_rag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretrieve_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3878839442.py\u001b[0m in \u001b[0;36mgenerate_response\u001b[0;34m(question, documents)\u001b[0m\n\u001b[1;32m     10\u001b[0m         {\n\u001b[1;32m     11\u001b[0m             \u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"system\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRAG_SYSTEM_PROMPT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         },\n\u001b[1;32m     14\u001b[0m         {\n",
            "\u001b[0;31mNameError\u001b[0m: name 'RAG_SYSTEM_PROMPT' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oezf8ikQFlGG"
      },
      "source": [
        "## wrap_openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYCKgbplFlGG"
      },
      "source": [
        "The wrap_openai/wrapOpenAI methods in Python/TypeScript allow you to wrap your OpenAI client in order to automatically log traces -- no decorator or function wrapping required! The wrapper works seamlessly with the @traceable decorator or traceable function and you can use both in the same application.\n",
        "\n",
        "You still need to set your `LANGSMITH_API_KEY` and `LANGSMITH_TRACING`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j70LQ-JzFlGH"
      },
      "source": [
        "![AWTT](../../images/alternative_ways_to_trace_3.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "nsdJbCI6FlGH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "02c7c29f-7ae7-49f7-e341-859af3a2ee28"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'utils'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2959999559.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnest_asyncio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_vector_db_retriever\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mMODEL_PROVIDER\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"openai\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Imports and Constants\n",
        "\n",
        "# TODO: Import wrap_openai from langsmith.wrappers when ready\n",
        "# from langsmith.wrappers import wrap_openai\n",
        "import openai\n",
        "from typing import List\n",
        "import nest_asyncio\n",
        "from utils import get_vector_db_retriever\n",
        "\n",
        "MODEL_PROVIDER = \"openai\"\n",
        "MODEL_NAME = \"gpt-4o-mini\"\n",
        "APP_VERSION = 1.0\n",
        "\n",
        "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the latest question in the conversation.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "Use three sentences maximum and keep the answer concise.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "wO7oitpgFlGH"
      },
      "outputs": [],
      "source": [
        "#  Client Initialization and Setup\n",
        "\n",
        "# TODO: Wrap the OpenAI Client once ready\n",
        "openai_client = openai.Client()\n",
        "\n",
        "nest_asyncio.apply()\n",
        "retriever = get_vector_db_retriever()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5zB6-fmFlGI"
      },
      "source": [
        "The wrapped OpenAI client accepts all the same langsmith_extra parameters as @traceable decorated functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "3jqNghuBFlGI"
      },
      "outputs": [],
      "source": [
        "#  Document Retriever Function\n",
        "\n",
        "from langsmith import traceable\n",
        "\n",
        "@traceable(run_type=\"chain\")\n",
        "def retrieve_documents(question: str):\n",
        "    return retriever.invoke(question)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "uBMJLM1XFlGI"
      },
      "source": [
        "## [Advanced] RunTree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhLfn6UzFlGJ"
      },
      "source": [
        "Another, more explicit way to log traces to LangSmith is via the RunTree API. This API allows you more control over your tracing - you can manually create runs and children runs to assemble your trace. You still need to set your `LANGSMITH_API_KEY`, but `LANGSMITH_TRACING` is not necessary for this method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gi8px35qFlGJ"
      },
      "source": [
        "![AWTT](../../images/alternative_ways_to_trace_4.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "hFXN03VJFlGJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "e3e682db-07b4-4013-8874-ebe3c22b2421"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'utils'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-582882565.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnest_asyncio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_vector_db_retriever\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Load environment variables from .env file if present\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Environment Setup and Imports\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langsmith import RunTree, utils\n",
        "from openai import OpenAI\n",
        "from typing import List\n",
        "import nest_asyncio\n",
        "from utils import get_vector_db_retriever\n",
        "\n",
        "# Load environment variables from .env file if present\n",
        "load_dotenv(dotenv_path=\"../../.env\", override=True)\n",
        "\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"false\"  # Disable auto-tracing, we use RunTree explicitly\n",
        "\n",
        "# Sanity check tracing status\n",
        "print(\"Tracing enabled?\", utils.tracing_is_enabled())  # Should print False\n",
        "\n",
        "openai_client = OpenAI()\n",
        "nest_asyncio.apply()\n",
        "retriever = get_vector_db_retriever()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5GdZS3jFlGK"
      },
      "outputs": [],
      "source": [
        "# Document Retrieval Function\n",
        "\n",
        "def retrieve_documents(parent_run: RunTree, question: str):\n",
        "    # Create a child run with added metadata\n",
        "    child_run = parent_run.create_child(\n",
        "        name=\"Retrieve Documents\",\n",
        "        run_type=\"retriever\",\n",
        "        inputs={\"question\": question},\n",
        "        metadata={\"version\": \"1.0\", \"component\": \"retriever\"}\n",
        "    )\n",
        "    documents = retriever.invoke(question)\n",
        "    print(f\"Documents retrieved: {[doc.page_content for doc in documents]}\")  # Debug log\n",
        "    child_run.end(outputs={\"documents\": documents})\n",
        "    child_run.post()\n",
        "    return documents\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciWG94f4FlGK"
      },
      "source": [
        "Let's go ahead and set `LANGSMITH_TRACING` to false, as we are using RunTree to manually create runs in this case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "92JA-lycFlGK"
      },
      "outputs": [],
      "source": [
        "# Response Generation Function with Document Formatting as Child Run\n",
        "\n",
        "def generate_response(parent_run: RunTree, question: str, documents, model: str = \"gpt-4o-mini\", temperature: float = 0.0):\n",
        "    # Split document formatting into own sub-run for extra granularity\n",
        "    prep_run = parent_run.create_child(\n",
        "        name=\"Format Documents\",\n",
        "        run_type=\"parser\",\n",
        "        inputs={\"documents\": documents},\n",
        "        metadata={\"version\": \"1.0\", \"component\": \"formatter\"}\n",
        "    )\n",
        "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
        "    prep_run.end(outputs={\"formatted_docs\": formatted_docs})\n",
        "    prep_run.post()\n",
        "\n",
        "    rag_system_prompt = \"\"\"You are an assistant for question-answering tasks.\n",
        "    Use the following pieces of retrieved context to answer the latest question in the conversation.\n",
        "    If you don't know the answer, just say that you don't know.\n",
        "    Use three sentences maximum and keep the answer concise.\n",
        "    \"\"\"\n",
        "    # Create a child run for the chain step with extra metadata and inputs\n",
        "    child_run = parent_run.create_child(\n",
        "        name=\"Generate Response\",\n",
        "        run_type=\"chain\",\n",
        "        inputs={\"question\": question, \"formatted_docs\": formatted_docs, \"model\": model, \"temperature\": temperature},\n",
        "        metadata={\"version\": \"1.0\", \"component\": \"chain\"}\n",
        "    )\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": rag_system_prompt\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
        "        }\n",
        "    ]\n",
        "    try:\n",
        "        openai_response = call_openai(child_run, messages, model=model, temperature=temperature)\n",
        "        child_run.end(outputs={\"openai_response\": openai_response})\n",
        "    except Exception as e:\n",
        "        child_run.end(outputs={\"error\": str(e)}, status=\"error\")\n",
        "        raise\n",
        "    child_run.post()\n",
        "    return openai_response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyClUxBTFlGK"
      },
      "source": [
        "We have rewritten our RAG application, except this time we pass a RunTree argument through our function calls, and create child runs at each layer. This gives our RunTree the same hierarchy that we were automatically able to establish with @traceable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "eO41XfylFlGL"
      },
      "outputs": [],
      "source": [
        "# OpenAI Call Function with Timing and Error Handling\n",
        "\n",
        "def call_openai(\n",
        "    parent_run: RunTree, messages: List[dict], model: str = \"gpt-4o-mini\", temperature: float = 0.0\n",
        ") -> str:\n",
        "    # Create a child run for the OpenAI call with metadata and timing\n",
        "    child_run = parent_run.create_child(\n",
        "        name=\"OpenAI Call\",\n",
        "        run_type=\"llm\",\n",
        "        inputs={\"messages\": messages, \"model\": model, \"temperature\": temperature},\n",
        "        metadata={\"version\": \"1.0\", \"component\": \"llm\"}\n",
        "    )\n",
        "    import time\n",
        "    start_time = time.time()\n",
        "\n",
        "    openai_response = openai_client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    child_run.end(outputs={\"openai_response\": openai_response, \"elapsed_time_secs\": elapsed_time})\n",
        "    child_run.post()\n",
        "    return openai_response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "A3nQYGbbFlGL"
      },
      "outputs": [],
      "source": [
        "# Root RAG Function with RunTree Management\n",
        "\n",
        "def langsmith_rag(question: str, model: str = \"gpt-4o-mini\", temperature: float = 0.0):\n",
        "    # Root RunTree with extra metadata and inputs, supports passing model params\n",
        "    root_run_tree = RunTree(\n",
        "        name=\"Chat Pipeline\",\n",
        "        run_type=\"chain\",\n",
        "        inputs={\"question\": question, \"model\": model, \"temperature\": temperature},\n",
        "        metadata={\"version\": \"1.0\", \"app\": \"langsmith_rag\"}\n",
        "    )\n",
        "\n",
        "    # Pass RunTree and params into calls\n",
        "    documents = retrieve_documents(root_run_tree, question)\n",
        "    response = generate_response(root_run_tree, question, documents, model=model, temperature=temperature)\n",
        "    output = response.choices[0].message.content\n",
        "\n",
        "    root_run_tree.end(outputs={\"generation\": output})\n",
        "    root_run_tree.post()\n",
        "    return output\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ls-academy",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}