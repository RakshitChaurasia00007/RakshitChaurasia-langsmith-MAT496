{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hk77hhNGQn_W"
      },
      "source": [
        "# Conversational Threads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5923IumZQn_f"
      },
      "source": [
        "Many LLM applications have a chatbot-like interface in which the user and the LLM application engage in a multi-turn conversation. In order to track these conversations, you can use the Threads feature in LangSmith.\n",
        "\n",
        "This is relevant to our RAG application, which should maintain context from prior conversations with users."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Uj3KhINQn_h"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "K8beHUaeQn_i"
      },
      "outputs": [],
      "source": [
        "# You can set them inline\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = \"\"\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy\"  # If you don't set this, traces will go to the Default project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJ7kpyekQn_k",
        "outputId": "5c6d9dc9-7c6a-4d97-a434-6ab86725f977"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Or you can use a .env file\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(dotenv_path=\"../../.env\", override=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wq5WrZabQn_k"
      },
      "source": [
        "### Group traces into threads\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pcqt0i5tQn_l"
      },
      "source": [
        "A Thread is a sequence of traces representing a single conversation. Each response is represented as its own trace, but these traces are linked together by being part of the same thread.\n",
        "\n",
        "To associate traces together, you need to pass in a special metadata key where the value is the unique identifier for that thread.\n",
        "\n",
        "The key value is the unique identifier for that conversation. The key name should be one of:\n",
        "\n",
        "- session_id\n",
        "- thread_id\n",
        "- conversation_id.\n",
        "\n",
        "The value should be a UUID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Yo7yu6ZfQn_l"
      },
      "outputs": [],
      "source": [
        " #Imports, Setup, and Initialization with Debug\n",
        "\n",
        "import uuid\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langsmith import traceable\n",
        "from openai import OpenAI\n",
        "from typing import List, Optional\n",
        "import nest_asyncio\n",
        "from utils import get_vector_db_retriever\n",
        "\n",
        "# Load environment variables from .env file for API keys, project and tracing flags\n",
        "load_dotenv(dotenv_path=\"../../.env\", override=True)\n",
        "\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"  # Enable tracing globally\n",
        "\n",
        "# Generate a unique thread_id for conversation linking\n",
        "thread_id = str(uuid.uuid4())\n",
        "print(f\"Generated thread_id for trace grouping: {thread_id}\")\n",
        "\n",
        "# Initialize OpenAI client and other dependencies\n",
        "openai_client = OpenAI()\n",
        "nest_asyncio.apply()\n",
        "retriever = get_vector_db_retriever()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "LB9pXZXoQn_m",
        "outputId": "3d5e8311-37b5-43ce-fad2-5d335ba4ae4b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Optional' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1621521852.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mtraceable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"chain\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mretrieve_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_metadata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Retrieving documents for question: {question}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Optional' is not defined"
          ]
        }
      ],
      "source": [
        "#  Document Retriever Function with Metadata and Error Handling\n",
        "\n",
        "@traceable(run_type=\"chain\")\n",
        "def retrieve_documents(question: str, extra_metadata: Optional[dict] = None):\n",
        "    print(f\"Retrieving documents for question: {question}\")\n",
        "    try:\n",
        "        docs = retriever.invoke(question)\n",
        "        print(f\"Retrieved {len(docs)} documents.\")\n",
        "    except Exception as ex:\n",
        "        print(f\"Error during retrieval: {ex}\")\n",
        "        docs = []\n",
        "\n",
        "    # Inject trace metadata for thread grouping if provided\n",
        "    metadata = {\"thread_id\": thread_id}\n",
        "    if extra_metadata:\n",
        "        metadata.update(extra_metadata)\n",
        "\n",
        "    # Return both documents and injected metadata for trace association\n",
        "    return {\"documents\": docs, \"metadata\": metadata}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQO6lSuSQn_o"
      },
      "source": [
        "### Now let's run our application twice with this thread_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGGYZ835Qn_p"
      },
      "outputs": [],
      "source": [
        "# Generate Response Function with Metadata, Debug, and Error Management\n",
        "\n",
        "@traceable(run_type=\"chain\")\n",
        "def generate_response(question: str, documents: List, extra_metadata: Optional[dict] = None):\n",
        "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
        "    print(f\"Formatting documents, length: {len(formatted_docs)} characters\")\n",
        "\n",
        "    rag_system_prompt = \"\"\"You are an assistant for question-answering tasks.\n",
        "    Use the following pieces of retrieved context to answer the latest question in the conversation.\n",
        "    If you don't know the answer, just say that you don't know.\n",
        "    Use three sentences maximum and keep the answer concise.\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": rag_system_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"}\n",
        "    ]\n",
        "\n",
        "    # Inject or update metadata with thread_id for tracing linkage\n",
        "    metadata = {\"thread_id\": thread_id}\n",
        "    if extra_metadata:\n",
        "        metadata.update(extra_metadata)\n",
        "\n",
        "    try:\n",
        "        response = call_openai(messages, langsmith_extra={\"metadata\": metadata})\n",
        "        print(\"OpenAI response received.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error calling OpenAI: {e}\")\n",
        "        response = None\n",
        "\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrNIfysXQn_p"
      },
      "outputs": [],
      "source": [
        "#  OpenAI Call Function with Metadata and Timing\n",
        "\n",
        "@traceable(run_type=\"llm\")\n",
        "def call_openai(messages: List[dict], model: str = \"gpt-4o-mini\", temperature: float = 0.0, langsmith_extra: Optional[dict] = None):\n",
        "    import time\n",
        "    print(f\"Calling OpenAI with model {model}...\")\n",
        "    start = time.time()\n",
        "\n",
        "    try:\n",
        "        response = openai_client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=messages,\n",
        "            temperature=temperature,\n",
        "            langsmith_extra=langsmith_extra\n",
        "        )\n",
        "        duration = time.time() - start\n",
        "        print(f\"OpenAI call duration: {duration:.2f} seconds\")\n",
        "    except Exception as error:\n",
        "        print(f\"OpenAI API call failed with error: {error}\")\n",
        "        raise\n",
        "\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bd4RGcV-Qn_q"
      },
      "source": [
        "### Let's take a look in LangSmith!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ls-academy",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}