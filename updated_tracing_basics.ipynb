{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ok_2TpepUvrl"
      },
      "source": [
        "# Tracing Basics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnbPs_eEUvro"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3H5SJ01Uvrp"
      },
      "source": [
        "Make sure you set your environment variables, including your OpenAI API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "4MY0NRQUUvrp"
      },
      "outputs": [],
      "source": [
        "# You can set them inline\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"OPEN AI API KEY\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = \"LANGSMITH API KEY\"\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjQyiXwOUvrr",
        "outputId": "60f3b7c9-0072-4c19-a166-f7a6b00f4d2e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# Or you can use a .env file\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(dotenv_path=\"../../.env\", override=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np29yqrAUvrr"
      },
      "source": [
        "### Tracing with @traceable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrIXYqakUvrs"
      },
      "source": [
        "The @traceable decorator is a simple way to log traces from the LangSmith Python SDK. Simply decorate any function with @traceable.\n",
        "\n",
        "The decorator works by creating a run tree for you each time the function is called and inserting it within the current trace. The function inputs, name, and other information is then streamed to LangSmith. If the function raises an error or if it returns a response, that information is also added to the tree, and updates are patched to LangSmith so you can detect and diagnose sources of errors. This is all done on a background thread to avoid blocking your app's execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "IuGqGPTEUvrt"
      },
      "outputs": [],
      "source": [
        "# utils.py\n",
        "# Simple dummy implementation for vector DB retriever (works in Google Colab)\n",
        "\n",
        "from typing import List\n",
        "\n",
        "class DummyDoc:\n",
        "    def __init__(self, page_content: str):\n",
        "        self.page_content = page_content\n",
        "\n",
        "\n",
        "class DummyRetriever:\n",
        "    \"\"\"\n",
        "    Simulates a vector DB retriever for testing RAG pipelines.\n",
        "    In real usage, replace with a LangChain retriever.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.documents = [\n",
        "            \"LangSmith helps trace and debug LLM applications.\",\n",
        "            \"OpenAI models like GPT-4o-mini can generate text and perform reasoning.\",\n",
        "            \"Vector databases are used in Retrieval-Augmented Generation (RAG) systems.\",\n",
        "            \"The @traceable decorator enables LangSmith function tracing.\",\n",
        "        ]\n",
        "\n",
        "    def invoke(self, query: str) -> List[DummyDoc]:\n",
        "        # Return any doc containing a query word\n",
        "        matched = [\n",
        "            DummyDoc(d)\n",
        "            for d in self.documents\n",
        "            if any(word.lower() in d.lower() for word in query.split())\n",
        "        ]\n",
        "        # If nothing matches, return all docs\n",
        "        return matched or [DummyDoc(d) for d in self.documents]\n",
        "\n",
        "\n",
        "def get_vector_db_retriever() -> DummyRetriever:\n",
        "    \"\"\"Return the dummy retriever instance.\"\"\"\n",
        "    return DummyRetriever()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8iMUeW1Uvrv"
      },
      "source": [
        "@traceable handles the RunTree lifecycle for you!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Imports ---\n",
        "from langsmith import traceable  # make sure langsmith SDK is installed\n",
        "from openai import OpenAI\n",
        "from typing import List\n",
        "import nest_asyncio\n",
        "\n",
        "# --- Dummy Retriever (utils.py functionality inline) ---\n",
        "class DummyDoc:\n",
        "    def __init__(self, page_content: str):\n",
        "        self.page_content = page_content\n",
        "\n",
        "class DummyRetriever:\n",
        "    def __init__(self):\n",
        "        self.documents = [\n",
        "            \"LangSmith helps trace and debug LLM applications.\",\n",
        "            \"OpenAI models like GPT-4o-mini can generate text and perform reasoning.\",\n",
        "            \"Vector databases are used in Retrieval-Augmented Generation (RAG) systems.\",\n",
        "            \"The @traceable decorator enables LangSmith function tracing.\",\n",
        "        ]\n",
        "\n",
        "    def invoke(self, query: str) -> List[DummyDoc]:\n",
        "        matched = [\n",
        "            DummyDoc(d)\n",
        "            for d in self.documents\n",
        "            if any(word.lower() in d.lower() for word in query.split())\n",
        "        ]\n",
        "        return matched or [DummyDoc(d) for d in self.documents]\n",
        "\n",
        "def get_vector_db_retriever() -> DummyRetriever:\n",
        "    return DummyRetriever()\n",
        "\n",
        "# --- Initialization ---\n",
        "openai_client = OpenAI()\n",
        "nest_asyncio.apply()\n",
        "retriever = get_vector_db_retriever()\n",
        "MODEL_NAME = \"gpt-4o-mini\"\n",
        "MODEL_PROVIDER = \"openai\"\n",
        "\n",
        "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the latest question concisely.\n",
        "\"\"\"\n",
        "\n",
        "# --- Functions with @traceable ---\n",
        "@traceable(metadata={\"vectordb\": \"dummy\"})\n",
        "def retrieve_documents(question: str):\n",
        "    return retriever.invoke(question)\n",
        "\n",
        "@traceable(metadata={\"model_name\": MODEL_NAME, \"model_provider\": MODEL_PROVIDER})\n",
        "def call_openai(messages: List[dict], model: str = MODEL_NAME, temperature: float = 0.0, langsmith_extra=None):\n",
        "    return openai_client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "\n",
        "@traceable\n",
        "def generate_response(question: str, documents, langsmith_extra=None):\n",
        "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": RAG_SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": f\"Context:\\n{formatted_docs}\\n\\nQuestion: {question}\"}\n",
        "    ]\n",
        "    return call_openai(messages, langsmith_extra=langsmith_extra)\n",
        "\n",
        "@traceable\n",
        "def langsmith_rag(question: str, langsmith_extra=None):\n",
        "    documents = retrieve_documents(question)\n",
        "    response = generate_response(question, documents, langsmith_extra=langsmith_extra)\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# --- Test queries ---\n",
        "question1 = \"How can I trace with the @traceable decorator?\"\n",
        "answer1 = langsmith_rag(question1)\n",
        "print(\"Answer 1:\", answer1)\n",
        "\n",
        "question2 = \"How do I add Metadata to a Run with @traceable?\"\n",
        "answer2 = langsmith_rag(question2)\n",
        "print(\"Answer 2:\", answer2)\n",
        "\n",
        "question3 = \"How do I add metadata at runtime?\"\n",
        "answer3 = langsmith_rag(question3, langsmith_extra={\"metadata\": {\"runtime_metadata\": \"foo\"}})\n",
        "print(\"Answer 3:\", answer3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lspZqjM8aVXR",
        "outputId": "2c2e4a92-0ea5-4896-87d7-52dfd8e04bb0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer 1: To trace with the @traceable decorator in LangSmith, you simply need to apply the decorator to the function you want to trace. This will enable function tracing, allowing you to monitor and debug the execution of that function within your LLM applications.\n",
            "Answer 2: To add metadata to a run with the @traceable decorator in LangSmith, you can use the `metadata` parameter within the decorator. Here’s a basic example:\n",
            "\n",
            "```python\n",
            "from langsmith import traceable\n",
            "\n",
            "@traceable(metadata={\"key\": \"value\"})\n",
            "def your_function():\n",
            "    # Your function implementation\n",
            "```\n",
            "\n",
            "Replace `{\"key\": \"value\"}` with your desired metadata. This will associate the specified metadata with the run when the function is executed.\n",
            "Answer 3: To add metadata at runtime in LangSmith, you can use the @traceable decorator, which allows you to trace functions and potentially include metadata during the tracing process.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQ6jiua-Uvrv",
        "outputId": "943627ad-3444-455d-834c-80abd48b2af2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To trace with the @traceable decorator in LangSmith, you would apply the decorator to the function you want to trace. This will enable function tracing, allowing you to monitor and debug the execution of that function within your LLM application.\n"
          ]
        }
      ],
      "source": [
        "question = \"How can I trace with the @traceable decorator?\"\n",
        "ai_answer = langsmith_rag(question)\n",
        "print(ai_answer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "carL-MEVUvrw"
      },
      "source": [
        "##### Let's take a look in LangSmith!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPTggDMYUvrx"
      },
      "source": [
        "### Adding Metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mdgxXe-Uvrx"
      },
      "source": [
        "LangSmith supports sending arbitrary metadata along with traces.\n",
        "\n",
        "Metadata is a collection of key-value pairs that can be attached to runs. Metadata can be used to store additional information about a run, such as the version of the application that generated the run, the environment in which the run was generated, or any other information that you want to associate with a run. Similar to tags, you can use metadata to filter runs in the LangSmith UI, and can be used to group runs together for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "jGu8YANPUvrx"
      },
      "outputs": [],
      "source": [
        "# utils.py\n",
        "from typing import List\n",
        "\n",
        "class DummyDoc:\n",
        "    def __init__(self, page_content: str):\n",
        "        self.page_content = page_content\n",
        "\n",
        "class DummyRetriever:\n",
        "    def __init__(self):\n",
        "        self.documents = [\n",
        "            \"LangSmith helps trace and debug LLM applications.\",\n",
        "            \"OpenAI models like GPT-4o-mini can generate text and perform reasoning.\",\n",
        "            \"Vector databases are used in Retrieval-Augmented Generation (RAG) systems.\",\n",
        "            \"The @traceable decorator enables LangSmith function tracing.\",\n",
        "        ]\n",
        "\n",
        "    def invoke(self, query: str) -> List[DummyDoc]:\n",
        "        matched = [\n",
        "            DummyDoc(d)\n",
        "            for d in self.documents\n",
        "            if any(word.lower() in d.lower() for word in query.split())\n",
        "        ]\n",
        "        return matched or [DummyDoc(d) for d in self.documents]\n",
        "\n",
        "def get_vector_db_retriever() -> DummyRetriever:\n",
        "    return DummyRetriever()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWp88M6xUvry",
        "outputId": "28dba710-32b1-4792-d5c1-16a3278e2562"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To add metadata to a run with the @traceable decorator in LangSmith, you can use the `metadata` parameter within the decorator. Here’s a basic example:\n",
            "\n",
            "```python\n",
            "from langsmith import traceable\n",
            "\n",
            "@traceable(metadata={\"key\": \"value\"})\n",
            "def my_function():\n",
            "    # Your function implementation\n",
            "    pass\n",
            "```\n",
            "\n",
            "Replace `{\"key\": \"value\"}` with your desired metadata. This will associate the specified metadata with the run when the function is executed.\n"
          ]
        }
      ],
      "source": [
        "question = \"How do I add Metadata to a Run with @traceable?\"\n",
        "ai_answer = langsmith_rag(question)\n",
        "print(ai_answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZhpVrTYUvry"
      },
      "source": [
        "You can also add metadata at runtime!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7JWZDSjUvry",
        "outputId": "44421a9d-a98f-4281-d71f-b8a07f1eb598"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To add metadata at runtime in LangSmith, you can use the `@traceable` decorator, which allows you to trace functions and include additional metadata during execution.\n"
          ]
        }
      ],
      "source": [
        "question = \"How do I add metadata at runtime?\"\n",
        "ai_answer = langsmith_rag(question, langsmith_extra={\"metadata\": {\"runtime_metadata\": \"foo\"}})\n",
        "print(ai_answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcEnhcGoUvry"
      },
      "source": [
        "##### Let's take a look in LangSmith!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ls-academy",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}